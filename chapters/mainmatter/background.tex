% !TeX root = ../../main.tex
\chapter{Background and Related Work}

\todo{Environment and platform treated as synonyms?}

This chapter aims to provide to the reader the necessary theoretical background behind the concepts of software portability, multi-platforms software development and interoperability between programming languages, unfolding how these concepts play a crucial role in the comprehensive software lifecycle and why they are relevant in modern software engineering.

\section{The role of portability in Software Engineering}

Software \textbf{portability} is, according to the ISO/IEC standard, the quality attribute that measures the \enquote{degree of effectiveness and efficiency with which a system, product or component can be transferred from one hardware, software or other operational or usage environment to another} \cite{iso-25010}.
%
Put differently, a software artifact is said to be portable, i.e. it exhibits portability, when the cost required to design and implement it for porting and cross-compiling for multiple platforms does not exceed the cost of re-development for each of them.
%
Portability is, therefore, a matter of engineering a software product to maximize the reusability of their components, guaranteeing the same behaviour and functionalities across the different targeted platforms, where the concept of platform is wide and embraces hardware architecture, operating system, runtime upon which the software is executed and, more generally, public interfaces on which application software depends \cite{Sommerville2020}.
%
Portability 
%
% TODO: se dico che è un attributo che può essere misurato, allora come ? See Mooney 2004
% is not a binary attribute, but rather a quantifiable degree that can be measured and evaluated with respect to a specific set of platforms and 
%
span over multiple levels \cite{mooney-2004} \todo{quantifiable? how?}:

\begin{itemize}
    \item \textbf{source portability} occurs when the software is adapted to the underlying platform by changing the source code, which is then recompiled for the target platform. This is the most common form of portability;
    \item \textbf{binary portability} involves porting software in its compiled binary format. This is the most advantageous form of portability, though it is also the most difficult to achieve and is limited to specific cases, for very similar environments;
    \item \textbf{intermediate-level portability} is a middle-ground between source and binary portability and entails porting an intermediate representation of the software sitting between the source and binary code. This is the case of modern languages with multi-target compilation \todo{cite next section where this is explained}.
\end{itemize}

Portability has always been a relevant concern in software engineering since the early days of computing when the landscape of hardware architectures and operating systems was extremely fragmented and heterogneous and software was completely tied to the underlying platform, requiring full rewrites when moving to a different one \todo{cite}.
%
Over time the situation has profoundly changed thanks to a series of innovations and efforts: from the standardization of operating systems interfaces, such as POSIX, to the widespread introduction of increasingly higher-level programming languages and the diffusion of modern paradigms, like the World Wide Web, that inherently fostered portability \todo{cite}.
%
Undoubtedly, two of the most influential shifts in this context are represented by the C language's compilation-based portability model and the subsequent virtual machine architectures, such as the Java Virtual Machine (JVM) and the Common Language Runtime (CLR) for .NET.
%
Despite being very different in nature, both these paradigms shifts lays their foundations in another cornerstone of software engineering: \textbf{abstraction}.
%
C addressed portability by abstracting hardware details through its type system, standard library and abstract machine semantics while preserving a direct mapping to machine operations and low-level control.
%
Despite it's early success, compilation-based portability faced inherint limitations: developers needs to create and mantain separate binaries for each target platform, which is a costly and painful process.
%
Virtual machine approaches emerged then addressing these challenges by introducing an additional abstraction layer between the software and the underlying platform: rather than compiling to platform-specific machine code, software is compiled to an intermediate bytecode that can be interpreted or dynamically compiled by a platform-specific runtime.
%
This approach has found widespread adoption, with the JVM and CLR becoming the backbone of entire ecosystems of modern languages and frameworks and enabling the famous \enquote{write once, run anywhere} paradigm $-$ the promise that applications would be portable across any platform supporting the respective runtime.
%
However, the technical reality proved to be more nuanced: different virtual machine implementations can exhibit subtle differences in behaviour and also system libraries can vary significantly across platforms, requiring additional adaptation efforts.
%
Contemporary portability solutions continue this abstraction progression, with modern languages like Kotlin and Scala, but also Gleam, Rust and others, supporting multi-target compilation to various platforms, including JVM, JavaScript, WebAssembly and native binaries using intermediate representations. \todo{Rust?? Gleam?? specify better relation with JVM}

However, despite the advancements in portability techniques and tools, achieving true portability remains a complex challenge as it requires ensuring consistent behavior across diverse platforms and environments that often differ greatly in their capabilities and constraints.
%
To illustrate, consider the differences between most common platforms in terms of threading models.
%
Nowadays, most applications are designed to take advantage of multi-core architectures and, therefore, heavily rely on concurrency to speed up computations or to handle multiple tasks simultaneously.
%
Nevertheless, not all platforms support these features equally.
%
For instance, the Node JS platform, widely used for server-side and web applications, is based on a single-threaded event loop model that makes of asynchronous, non-blocking I/O operations its core feature.
%
While this model is highly efficient for I/O-bound tasks, it can pose significant challenges for CPU-bound operations that require parallel processing.
%
Also the Python platform, despite having multi-threading support, in CPU-bound tasks is limited by the Global Interpreter Lock (GIL), which allows only one thread to execute Python bytecode at a time, effectively serializing multi-threaded CPU-bound operations.

\todo{does it affect portability? or just performance?}

Moreover, the embedded systems and IoT domains demonstrate that abstraction has practical limits in contexts where resource-constrained devices with minimal RAM and flash memory cannot support virtual machine overhead or large bundles, necessitating low-level compilation-based approaches.

\todo{Move above?}

Despite the challenges, portability remains a crucial aspect of modern software engineering, driven by the need to reach heterogeneous infrastructures and adapt to the rapidly evolving technological landscape, reducing for the developers the burden of maintaining or re-implementing software for different platforms, which often leads to fragmentation, inconsistencies between the different versions and increased maintenance costs.

All of this at what cost?

Portability tremendously impacts the whole software lifecycle, from design to release and maintenance.

\vspace{0.5em}
\noindent
\textbf{Design and implementation.}
%
When designing a new software product the information about the targeted platforms is an essential input for the architect and the designer as they need to take into account the constraints and capabilities of the different environments in which the software will integrate with the primary goal of reducing the \textbf{abstraction gap}, i.e., the space among the domain problem to be faced and the abstractions and capabilities offered by a platform \todo{cite}.
%
The greater the abstraction gap is, the more challenging it becomes to implement the required functionalities in a clean, well structured and maintainable way.
%
In this context, the maturity of the ecosystem of the targeted platform and the availability of libraries and frameworks may reduce significantly the abstraction gap and, therefore, undoubtedly influence the platform choice.
%
This has a great impact also during the implementation, as the availability of maintained, well-tested and performing libraries and frameworks allows developers to focus on the core functionalities of the application and reuse existing components rather than re-implementing them from scratch, with all the drawbacks in terms of development time, costs and potential bugs that this entails.

For example, if a software product requires advanced data analysis and machine learning capabilities, targeting a platform with a rich ecosystem of libraries and frameworks in this domain, such as Python, would be a wise choice.

(from \cite{doeraene-2018} "In practice, even if the language itself is portable, unless the ecosystem follows suit and publishes portable libraries, it remains difficult to do any meaningful job spanning several runtimes.")

\vspace{0.5em}
\noindent
\textbf{Testing.}
%
Having a multi-platform stack significantly impacts the testing strategy since the artifacts need to be tested on each of the targeted platforms to ensure that they behave correctly and consistently across them.

\vspace{0.5em}

\noindent
\textbf{Release.}
%
Releasing a multi-platform software product is much more complex due to the fact every platform has its own conventions and requirements for packaging, distributing and deploying software.
%
For example, JVM-based artifacts are distributed as JAR via Maven central or other Maven repositories, JavaScript applications are often bundled using tools like Webpack and distributed via npm, while Python packages are typically distributed as wheels via PyPI.
%
Each of these release platforms have different policies and requirements in terms of publishing and versioning that need to be properly addressed.

\section{Language interoperability}

Portability is only one side of the coin.
%
Orthogonally to the platform dimension, language interoperability is equally important as it refers to the ability to communicate with other languages on each targeted platform, guaranteeing uniformity in semantics across them \cite{doeraene-2018}.
%
Having portability without interoperability is of limited practical use: while portability make it possible to cross-compile the artifacts for different platforms, the lack of interoperability with the host platform APIs and ecosystem's libraries would make the software product isolated and unable to interact with the surrounding ecosystem.
%
For example, developing an application targeting the JavaScript platform without being able to access JavaScript platform-specific functionalities, such as manipulating the DOM, handling user events or performing filesystem operations leveraging Node.js API would make portability largely theoretical rather than practical.

Interoperability, however, is not only the requirement of being able to access platform-specific APIs, but, rather, as library developers, the commitment to expose the library APIs to the platform native language of reference.
%
This enables the widest possible audience of developers to use the library and, most importantly, to let them integrate it with the full power of the underlying platform ecosystem.
%
This is particularly relevant because the various platforms ecosystems have been evolved in silos over the years, they often lack interoperability with each other, and each of them is focused and optimized for specific domains and use cases.
%
Language interoperability therefore allows, together with portability, developers to choose the best platform and, consequently, the best library API for their specific needs, minimizing the risk of incurring in incompatibility issues.

Crafting a "good" interoperable layer is a complex task whose main difficulty lays in the mismatch between the run-time semantics and abstractions of the languages involved and that includes, among others, dealing with different memory management strategies (e.g., garbage collection vs manual memory management), type systems (e.g., static vs dynamic typing, strong vs weak typing), error handling mechanisms (e.g., exceptions vs error codes). 
%
Sometimes, these differences cannot be fully bridged and some features can be dropped or limited if they cannot be mapped to the target language and are not essential for the library's core functionalities, but this must be carefully evaluated on a case-by-case basis.
%
However, the key point about interoperability is that the features must be complete with the respect to the semantics of the host language \cite{doeraene-2018}.
%
This ensures to be able to fully leverage the host language ecosystem and, therefore, to integrate seamlessly with other libraries and frameworks.

\section{Approaches to multi-platform and polyglotism}

Industrial frameworks that have achieved maturity and widespread adoption in addressing multi-platform development and polyglotism can be classified in \textit{cross-compilation} and \textit{wrapper-based} approaches.

In the cross-compilation approach, the source code is written in a sort of \enquote{superset} language that is automatically compiled and/or transpiled to different platform-specific targets thanks to a dedicated toolchain, generating platform-specific artifacts.
%
In this approach the source language itself is designed to provide both portability and interoperability with each target platform and is referred as \textit{cross-platform language} \cite{doeraene-2018}.
%
Despite having a single super language that can be cross-compiled to different targets, that doesn't mean that all the code can be shared across all the platforms.
%
Thus, when embracing this approach the main goal is to design the software product to maximize the reusability of its components across the different targeted platforms and fill the abstraction gap with the underlying platforms via  minimal platform-specific code leveraging, whenever possible, cross-platform libraries.
%
Examples of cross-platform languages include, among others, Kotlin, Scala and Flutter/Dart \todo{cite}.
%
The primary advantage of this approach is that it enables sharing a substantial portion of the codebase across different platforms.
%
Indeed, typically, most of the core business logic $-$ the part of the software that delivers the actual value $-$ since it is designed to be technologically agnostic can be for the most part, if not completely, shared across all the platforms.
%
In this respect, striving to maximize code sharing, though it may require more effort initially, pays off in the long run as it is synonymous of better design, with all the benefits that this entails in terms of software engineering.

The other approach is the wrapper-based one and consist in writing the software product in a main platform and related language and the exposing its functionalities to other platforms and languages via a dedicated interoperability layer, typically implemented as a wrapper library, through \textit{Foreign Function Interfaces} (FFI).
%
These interfaces requires dedicated runtime support: for example, many interpreted languages, such as Python, include mechanisms for executing extensions modules written in lower-level languages like C or C++ that are dynamically loaded into the high-level languages's virtual machine during execution \cite{Grimmer_2018}.
%
During the years, automated binding generation tools have been developed to ease the creation of the interoperability layers.
%
One very famous example is the Simplified Wrapper and Interaface Generator (SWIG) \todo{cite}, which can generate language bindings from C and C++ libraries' headers files to a variety of high-level languages, including Python, Java, JavaScript, Ruby and others.
%
Differently from the cross-compilation approach, in the absence of automated tools, the wrapper libraries need to be manually implemented, which can be a complex and error-prone task.

\subsection{The case of Scala}

Scala is a modern multi-paradigm programming language seamlessly integrating both object-oriented and functional programming, whose powerful static type system and advanced language abstraction features make it suitable for crafting complex and maintainable software systems in a concise, elegant and expressive way.
%
Originally designed to run on the Java Virtual Machine (JVM), Scala has evolved to support also native and JavaScript platforms making it a cross-platform language.
%
Despite other languages, Scala cross-platform capabilities are not tied to the language itself, but rather are demanded by dedicated projects, each with its own compiler plugins and toolchains that extends the Scala compiler and ecosystem.

\subsubsection{Scala.js}


\subsubsection{Scala Native}

Following the success of Scala.js, the Scala Native project moved their first steps in 2014 as a research project at EPFL with the goal of compiling Scala code directly to bare-metal machine code without the need for the JVM, making it suitable for resource-constrained environments where the overhead imposed by the JVM in terms of memory footprint and startup time cannot be tolerated.
%
Since it's 0.5 release published in 2024 with the introduction of native concurrency primitives, the project have reached a decent level of maturity and stability, that allows to consider it for production use.

Scala Native is both an \textit{ahead of time} (AOT) compiler and standalone runtime.

The AOT compiler translates Scala code into native machine code leveraging the LLVM toolchain \todo{cite LLVM}, a widely adopted modular compiler infrastructure for producing optimized machine code for multiple architectures. 
%
Compilation proceeds through a pipeline of stages, shown in \Cref{fig:scala-native-compiler-pipeline}, performed after the Scala frontend compiler (responsible for parsing and type-checking it) \todo{cite here or in image}:

\begin{itemize}
    \item the first stage of the pipeline is performed by the \texttt{nscplugin} that, inspecting the abstract syntax tree, translates the Scala code into a strongly typed Native Intermediate Representation (NIR) consisting of a subset of LLVM IR instructions enriched with additional information to support Scala high level abstractions;
    \item NIR files are then linked together with those generated from external libraries and optimized;
    \item finally, the optimized NIR is translated to low-level LLVM instructions that are then compiled by the LLVM backend to platform-specific machine code.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{resources/img/native-pipeline.pdf}
    \caption{Scala Native simplified compiler pipeline.}
    \label{fig:scala-native-compiler-pipeline}
\end{figure}

Other than the compiler itself, Scala Native includes a lightweight runtime layer that supplies essential services for the program execution.
%
Most notable features provided by the runtime are:

\begin{itemize}
    \item a configurable garbage collectore for automatic memory management of heap-allocated Scala objects;
    \item a threading model based on native OS threads that maps Scala's concurrency abstractions to the underlying platform capabilities trying to reach maximum compatibility in semantics with the JVM;
    \item foreign function and a subset of C interoperability primitives to interface with C or C++ existing libraries and native code, allowing to expose Scala Native libraries to native interoperable languages;
    \item a subset of the Scala and Java standard libraries re-implemented to work natively. In this way the developer can leverage familiar APIs and use existing Scala and Java standard libraries like it was running on the JVM with the same semantics (Scala Native consider, indeed, every difference in semantics between the two platforms as a bug).
\end{itemize}

Concerning the supported architectures and operating systems, Scala Native can target AMD64 (x86-64) and ARM64 (aarch64) architectures on Linux, macOS and Windows operating systems, also thanks to cross-compilation support \todo{what is it?}.
%
This covers the vast majority of modern desktop and server environments, including some System-on-Chip (SoC) architectures, like Raspberry Pi devices.
%
Experimental or partial support exists for FreeBSD, OpenBSD, and NetBSD, though sometimes limited to particular architectures (e.g. only AMD64 on OpenBSD). 
%
Moreover, with the 0.5 release, Scala Native introduced experimental support for 32-bit architectures, like ARMv7.
%
Mobile platforms (such as Apple devices) and microcontroller architectures (like ESP32 architecture) are not supported: the first due to the lack of support for Objective-C or Swift interoperability, which are essential for iOS and macOS development, and the latter because of the limited resources of these devices.
%
While the first limitation could be overcome in the future, the second will be hardly addressed because of the usage of limited or not fully mature LLVM support and the fact that compiled executable include a runtime layer that, alone, requires a few megabytes of memory that typically microcontrollers cannot accomodate.

\vspace{1em}

\todo{In the end remember to discuss adoption of cross-platform libraries in the Scala ecosystem.}



% Why choosing Scala as a language as a reference? The role of abstraction in software engineering
